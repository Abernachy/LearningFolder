{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web_Scraping_guides.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTQbiqNQLR5D"
      },
      "source": [
        "## Web Scraping\n",
        "\n",
        "\n",
        "Resources and shit:\n",
        "1. https://realpython.com/python-requests/\n",
        "2. https://rapidapi.com/blog/how-to-use-an-api-with-python/\n",
        "3. https://docs.python-guide.org/scenarios/scrape/\n",
        "4. https://requests.readthedocs.io/en/master/\n",
        "5. https://www.dataquest.io/blog/web-scraping-tutorial-python/\n",
        "6. https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
        "7. https://realpython.com/python-json/\n",
        "\n",
        "\n",
        "Since web pages are built as a structured document, we can use web scraping techniques to sift through a web page and grab data in a format we want"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r6ggmLKLR5N"
      },
      "source": [
        "### Components of a page\n",
        "Source: https://www.dataquest.io/blog/web-scraping-tutorial-python/\n",
        "\n",
        "\n",
        "So a web page is essentially composed of 3 fucking components:\n",
        "\n",
        "1. HTML - the main content of a page\n",
        "2. CSS - the sytling to make the page look nicer\n",
        "3. js - the Javascript to add interactivity to the web pages.\n",
        "\n",
        "It's like a fucking body.  JS is the circulatory system and the brain, CSS is the muscle, and HTML is the skin\n",
        "\n",
        "After the browser recieves all 3 components (files) it renders the page and displays it to us.  \n",
        "\n",
        "### HTML -> Hyper Text Markup Language\n",
        "Basically the language the web pages are created in.  \n",
        "Easiest way to see this shit is to inspect a web page (chrome: right click-> inspect) and you'll see tags like <html></html>,etc<div<>/div>, etc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eoRby8LLR5G"
      },
      "source": [
        "## API requests\n",
        "\n",
        "\n",
        "\n",
        "So typically when we use the requests module we can make a requests to the site we are looking to scrape from.\n",
        "\n",
        "When making a request we basically can make one of 4 types of actions:\n",
        "\n",
        "1. GET - retrieves information.  This is the most common type, as using it we can get data we are interested in.\n",
        "\n",
        "2. POST - adds new data to the server.  This could add new items to an inventory\n",
        "\n",
        "3. PUT - changes existing information.  Doing this could change the color or value of an existing product\n",
        "\n",
        "4. DELETE - deletes the existing information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg6vFauhLR5H",
        "outputId": "f056f86b-e43f-4042-bfa2-564e53007c55"
      },
      "source": [
        "# Doing a blank request to show what we can we can get back:\n",
        "\n",
        "import requests\n",
        "response = requests.get('https://google.com')\n",
        "print(response)\n",
        "# This will generate a value of 200"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Response [200]>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA6rhWnwLR5I"
      },
      "source": [
        "### The Status codes\n",
        " Status codes are returned with a response call. There are a number of options you can get:\n",
        " \n",
        " 1. 200 - OK - Request was successfull\n",
        " 2. 204 - No Content, requests worked but nothing was returned\n",
        " 3.  301 - Moved permanently - Server responds that the requested target has been moved to another address and redirects to this address\n",
        " 4. 400 - Bad Request - server cannot process the request because client-side errors (you fucked up)\n",
        " 5. 403 - forbidden - Access to the specified resource is denied\n",
        " 6. 404 - Not Found - Requested resource was not found\n",
        " 7. 500 - Internal Server Error - Unknown error has occurred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hof0a92WLR5J"
      },
      "source": [
        "### EndPoint\n",
        "API (Application Program Interface) permits the interaction between 2 systems.  \n",
        "\n",
        "An Endpoint, is the point of entry in a communication channel when 2 systems are interacting.  It refers to touchpoints of the communication between an API and a server.  This can be viewed as the means from which an API can access the resources they need from a server to perform their task.  \n",
        "\n",
        "When an API requests access to data on a web application or server, the response is sent back.  The location **where the api sends a requests and where the response emanantes is called the endpoint** \n",
        "\n",
        "### Security:  (Post thoughts, Is this important?)\n",
        "\n",
        "Endpoints are normally open but can be secured through a few ways:\n",
        "\n",
        "| Method    | Description |\n",
        "| --------  | ---------- |\n",
        "| 1 One Way password| -Store the password using one way or asymmetric encryption|\n",
        "| 2 Make HTTPS the only option | Use https for your GET request|\n",
        "| 3 Institute Rate Limiting |  Enforcing a limit of how many requests a customer can make use of an API can help discourage bots and avoid unnecessary use of system resources|\n",
        "| Solid Authentication |  Use a solid authentication technique like Oath2 (It's now the industry prefered) |\n",
        "| 5 Input validation is crucial | validating user inputs help decipher and identify threats before they reach the client.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldr-1zgXXajr"
      },
      "source": [
        "## JSON\n",
        "\n",
        "Jabascrip Object Notation is a standard for how data is saved on a page.  We see it in Python as dictionaries.  \n",
        "\n",
        "Python has a package for natively supporting JSON encoding and decoding JSON data using the JSON module.\n",
        "\n",
        "How it works is that the module takes serialized (data transformed into bytes) and deserializes it (decodeS) into data stored into the JSON standard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWkBf7ICYrKi",
        "outputId": "d537aa92-b648-44f0-8045-d00219161f54"
      },
      "source": [
        "# Example:\n",
        "\n",
        "import json, os\n",
        "\n",
        "\n",
        "data = { \"president\":{\n",
        "    \"name\": \"Zap Branigan\",\n",
        "    \"Occupation\": \"Sexy President\"\n",
        "  }\n",
        "}\n",
        "with open(\"data_file.json\",\"w\") as write_file:\n",
        "  json.dump(data, write_file)\n",
        "\n",
        "json_string = json.dumps(data)\n",
        "\n",
        "# outputs the deserialized data\n",
        "json.dumps(data)\n",
        "\n",
        "# does the same, but makes it more..readable\n",
        "json.dumps(data, separators=(', ', ':'))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{\"president\":{\"name\":\"Zap Branigan\", \"Occupation\":\"Sexy President\"}}'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cluiVLa0ciOa"
      },
      "source": [
        "### JSON Kewywords and things\n",
        "\n",
        "So some stuff about the above, json.dump() takes 2 arguments: (1) the data object to be serialized, and (2) the file like object to which the bytes will be written.\n",
        "\n",
        "\n",
        "Using the json.dumps(data) like above, we return it as a string object.  That being said, squishes the data together, so we want to set up some whitespace.\n",
        "\n",
        "Below, we use the example to take in serialized data and deserialize it to a string we can use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPxUo1mTiArU"
      },
      "source": [
        "# Deserialization Example, this will pull our wild data and \n",
        "with open(\"data_file.json\",\"r\") as read_file:\n",
        "  data = json.load(read_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uLNuoWdjrHY"
      },
      "source": [
        "### Real World Example using JSONPlaceholder\n",
        "\n",
        " Site:  https://jsonplaceholder.typicode.com/\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4CBwMtZj7r5",
        "outputId": "3b90752f-e164-4a12-b388-f33e41f9cfd0"
      },
      "source": [
        "import json, requests\n",
        "\n",
        "# Make a request to the placeholder api\n",
        "response = requests.get(\"https://jsonplaceholder.typicode.com/todos\")\n",
        "todos=json.loads(response.text)\n",
        "\n",
        "# Now todos is essentially a list of the individual JSON items that were separated by a comma\n",
        "print(todos[0])\n",
        "\n",
        "# So now lets have some fun.  Each userID has a comnleted property marked with a boolean\n",
        "# Lets see if we can determine which users have completed the most task\n",
        "\n",
        "# Map the userID to number of completed TODOS for that user\n",
        "todos_by_user = {}\n",
        "\n",
        "# incrememnt the complete TODOS count for each users\n",
        "for todo in todos:\n",
        "  if todo[\"completed\"]:\n",
        "    try:\n",
        "      # incremement the existing user's count\n",
        "      todos_by_user[todo[\"userId\"]]+=1\n",
        "    except KeyError:\n",
        "      # This user has not been seen.  Set their count to 1\n",
        "      todos_by_user[todo[\"userId\"]]= 1\n",
        "\n",
        "print(todos_by_user)\n",
        "#  Create a sorted list of (userID, num_complete) pairs\n",
        "\"\"\"\n",
        " NOTE: At the time of this shit I didn't have much experience with sorted() function\n",
        " key=lambda x:x[1] refers to sorting the items based on the value of the userID Key\n",
        " reverse = True has me grab the highest value amongst the Key Value pairs\n",
        " source: https://docs.python.org/3/howto/sorting.html\n",
        "\"\"\"\n",
        "top_users = sorted(todos_by_user.items(),\n",
        "                   key=lambda x: x[1], reverse=True)\n",
        "\n",
        "\n",
        "# Get the maximum number of completed TODOS from the sorted list\n",
        "# selects the first list object, second item in this list of tuples\n",
        "max_complete = top_users[0][1] \n",
        "\n",
        "\n",
        "# create a list of all users who have completed the max number of TODOS\n",
        "users = []\n",
        "for user, num_complete in top_users:\n",
        "  if num_complete < max_complete:\n",
        "    break\n",
        "  users.append(str(user))\n",
        "\n",
        "\n",
        "max_users = \" and \".join(users)\n",
        "\n",
        "s = \"s\" if len(users) > 1 else \"\"\n",
        "print(f\"user{s} {max_users} completed {max_complete} TODOs\")\n",
        "\n",
        "\n",
        "# Now lets continue with the guide\n",
        "# Define a function to filter out completed TODOs\n",
        "# of users with max completed TODOs\n",
        "\n",
        "def keep(todo):\n",
        "  is_complete = todo[\"completed\"]\n",
        "  has_max_count = str(todo[\"userId\"]) in users\n",
        "  return is_complete and has_max_count\n",
        "\n",
        "\n",
        "# Write filtered TODOs to file\n",
        "\"\"\"\n",
        "NOTE: This cell was copied to an offline .py file so it would create a file on my \n",
        "local system.  This guide is written and set up for use on Colab so local shit\n",
        "is not the preference.  \n",
        "\n",
        "This created a json file as output showing the top userIDs and what they completed\n",
        "\n",
        "with open(\"filtered_data_file.json\", \"w\") as data_file:\n",
        "  filtered_todos = list(filter(keep, todos))\n",
        "  json.dump(filtered_todos, data_file, indent=2)\n",
        "\"\"\"\n",
        "\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'userId': 1, 'id': 1, 'title': 'delectus aut autem', 'completed': False}\n",
            "{1: 11, 2: 8, 3: 7, 4: 6, 5: 12, 6: 6, 7: 9, 8: 11, 9: 8, 10: 12}\n",
            "users 5 and 10 completed 12 TODOs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VzosHQl8aO4"
      },
      "source": [
        "### Encoding and Decoding custom data \n",
        "\n",
        "So, if you have data that is encoded outside the norm (example: a class object) , it will complain that the object is not serializable.\n",
        "\n",
        "While one method could be to encode and decode by hand, you can throw the custom data at the JSON deserializer while adding in a middle step.\n",
        "\n",
        "Before you decode it, you put the data in a state that the built in types JSON already understands.  You basically translate the more complex objects into a simpler represenations.\n",
        "\n",
        "there's a complex object you can play with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QkyohyC_Btc",
        "outputId": "beae2168-ea79-4390-f12d-06cb5a096288"
      },
      "source": [
        "z = 3 + 8j\n",
        "type(z)\n",
        "#json.dumps(z)\n",
        "\n",
        "# So..what is the minimum amount of information necessary to recreate the object\n",
        "# For complex numbers you only need to know the real and imaginary parts\n",
        "\n",
        "print(z.real) # prints 3.0\n",
        "print(z.imag) # prints 8\n",
        "\n",
        "# If we pass those numbers into a complex constructor..it will satisfy the comparison operator\n",
        "complex(3,8) == z"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.0\n",
            "8.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2xDtGtWQbMR"
      },
      "source": [
        "### Encoding Custom Types\n",
        "\n",
        "To translate a custom object into a JSON, all you have to do is provide an encoding function to the dump() method's deault parameter.  The  json module will call this function on any objects that aren't natively serializable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Pmnt1w82RjvU",
        "outputId": "645450a7-8e67-4d73-e86b-c8305f81a103"
      },
      "source": [
        "# Example\n",
        "def encode_complex(z):\n",
        "  if isinstance(z, complex):\n",
        "    return (z.real, z.imag)\n",
        "  else:\n",
        "    type_name = z.__class__.__name__\n",
        "    raise TypeError(f\"Object of type '{type_name}' is not JSON serializable\")\n",
        "\n",
        "json.dumps(9+5j, default=encode_complex )\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[9.0, 5.0]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "RCSSa2jOWSn2",
        "outputId": "045535ee-79d0-4618-e4a1-521a395170a9"
      },
      "source": [
        "# Alternative method is to subclass the standard JSONEncoder and \n",
        "# override its default method\n",
        "\n",
        "class ComplexEncoder(json.JSONEncoder):\n",
        "  def default(self,z):\n",
        "    if isinstance(z,complex):\n",
        "      return (z.real, z.imag)\n",
        "\n",
        "json.dumps(2+5j, cls=ComplexEncoder)\n",
        "encoder = ComplexEncoder()\n",
        "encoder.encode(3+6j)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[3.0, 6.0]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSXBIJ-Hgkk_"
      },
      "source": [
        "### Decoding Custom Types\n",
        "When you try to encode a complex number with the ComplexEncoder and then decode it, you end up getting a list back.\n",
        "\n",
        "You would normally have to pass the values into a comnplex constructor if you wanted the complex object again.\n",
        "\n",
        "\n",
        "NOTE: This whole section has been basically Japanese to me.  Seeing as I am not at that point for encoding and decoding my own custom objects, I'll stop here.  I will revisit in time when I get a better knowledge base."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5otnPm-hNpG"
      },
      "source": [
        "### TLDR:\n",
        "\n",
        "Using JSON module with GET request allows you to scrape and fuck with metadata.  Using some of this stuff you can do things like:\n",
        "\n",
        "1. Import a JSON package\n",
        "2. Read the Data with load() / loads()\n",
        "3. Process the data\n",
        "4. Write the altered data with dump() /dumps.\n",
        "\n",
        "\n",
        "Once the data is loaded into memory its use is based on what you want it to be.  Typically a goal is to gather it from a source, extract useful information, and pass the information or keep a record of it.\n",
        "\n",
        "\n",
        " **Future Options**: pickle and marshal modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY0syOBzLR5L"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "### Bad Dragon API\n",
        "\n",
        "So the guide that exist on the python site points to a nonexitent API, fucking piece of shit.\n",
        "\n",
        "Fortunately...I know of an  API I can use.  \n",
        "\n",
        "Bad Dragon sells adult sex toys designed for either folks of culture, or fucked up individuals like myself.  They have their API which when you go directly to it, is a list of dictionaries, so..lets fuck with that shit and see what we can do.\n",
        "\n",
        "Now to find their APIs, I initially got lucky with a random ass google search.  However; another method I found to find more APIs I can make request to was to go to their main page.\n",
        "\n",
        "1. From the main page Inspect it like a dirty whore\n",
        "\n",
        "2.  Go to the Network and filter so that only XHR content is viewable/\n",
        "\n",
        "3.  The name column will list all of their APIs you can make get request to: home, dynamic-shop-text (such dirty text), products, etc.  Curiously, they also have a /api/sale , but due to COVID being a bitch, their sales are cancelled.  Their drops (clearances) are generated from folks who order custom made toys like a horse dick the color of a Mario Bros pipe who then cancel it before it ships.\n",
        "\n",
        "\n",
        "** Ignore this bit , still trying to figure Where the fuck to put this shit here **\n",
        "| Method | Description|\n",
        "|--------|------------|\n",
        "| .headers | returns the headers of the site, which are the technical data, not the content|\n",
        "| .content| returns all of the content within the api|\n",
        "----------------------------------------\n",
        "\n",
        "Site: https://bad-dragon.com/api/products\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "kwVtAolMLR5L",
        "outputId": "21517672-600f-4159-c36d-cfda3df1051f"
      },
      "source": [
        "import requests, json\n",
        "\n",
        "\n",
        "bad = requests.get('https://bad-dragon.com/api/products')\n",
        "bad.status_code\n",
        "# Positive hit..200\n",
        "\n",
        "naughtyjson = json.loads(bad.text)\n",
        "naughtyjson[0]\n",
        "# Sweet, lets see some shit with this.\n",
        "\"\"\"\n",
        "So, if you run the above you'll see each item is a dictionary:\n",
        "\"sku\": 'teenie lights'\n",
        "'name': 'Teenie Lights'\n",
        "'description': ** I'm not typing that**\n",
        "etc and shit\n",
        "\n",
        "I also got lazy figuring out what each position should so if you want to call\n",
        "specific item numbers.  Just run this:\n",
        "\n",
        "elementcount = 0\n",
        "for element in naughtyjson[0]:\n",
        "  print(f'Element Name: {element}, Position: {elementcount}')\n",
        "  elementcount+=1\n",
        "\"\"\"\n",
        "# So I got some sweet data..what can I do with this?\n",
        "# \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nSo, if you run the above you\\'ll see each item is a dictionary:\\n\"sku\": \\'teenie lights\\'\\n\\'name\\': \\'Teenie Lights\\'\\n\\'description\\': ** I\\'m not typing that**\\netc and shit\\n\\nI also got lazy figuring out what each position should so if you want to call\\nspecific item numbers.  Just run this:\\n\\nelementcount = 0\\nfor element in naughtyjson[0]:\\n  print(f\\'Element Name: {element}, Position: {elementcount}\\')\\n  elementcount+=1\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmeB_ygjLR5N"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxrGpcVlpTBV"
      },
      "source": [
        "### Future options For this block or even considerations\n",
        "\n",
        " - Final Fantasy 14 Market Bot\n",
        " - Fairy Lust also sells the same style and design of sex toys, poke around\n",
        " - Wallstreet Bets and Google Finance to get financial data ideas\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPpQep0JLR5N"
      },
      "source": [
        "## LXML\n",
        " \n",
        "    lxml is a library for parsing html and HTML documents very quickly, and can be used in conjuction with the Request Module\n",
        "    \n",
        "    Both modules will need to be installed in order to make use of them.  If you have an anaconda installation, these modules are installed by defaul\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfAUpUgSLR5O"
      },
      "source": [
        "from lxml import html\n",
        "from pathlib import Path\n",
        "import requests, os\n",
        "\n",
        "# Snag a web page with the data and parse it using the html module, saving the results as in a tree\n",
        "\n",
        "page = requests.get('http://econpy.pythonanywhere.com/ex/001.html')\n",
        "tree = html.fromstring(page.content)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beKN3qlILR5O"
      },
      "source": [
        "tree contains a whole HMTL file in a nice tree structure, which can be looked at 2 different ways: XPath and CSSSelect.  \n",
        "\n",
        "XPath is a way to locate information in a structure document such as HTML or XML\n",
        "\n",
        "To snag the XPath in Chrome, you can inspect the element of the page, find the code you want, right click -> copy xpath\n",
        "\n",
        "Going to the econpy site, we can see that each element is broken down by <div title= \"buyer-name\">\"The item name\"</div>\n",
        "<span class = \"item-price\">\"Das price\"</span>\n",
        "\n",
        "So, knowing this, the correct XPath query can be snagged, and using the lxml xpath function:\n",
        "\n",
        "## This will create a list of buyers:\n",
        "buyers = tree.xpath('//div[@title=\"buyer-name\"]/text()')\n",
        "## This will create a list of prices\n",
        "prices = tree.xpath('//span[@class=\"item-price\"]/text()')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJrcOPzFMeV1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRdxlbj8LR5P"
      },
      "source": [
        "Selecting Nodes\n",
        "XPath uses path expressions to select nodes in an XML document. The node is selected by following a path or steps. The most useful path expressions are listed below:\n",
        "\n",
        "|Expression\t|Description|\n",
        "\n",
        "|nodename\t|Selects all nodes with the name \"nodename\"|\n",
        "\n",
        "/\tSelects from the root node\n",
        "\n",
        "//\tSelects nodes in the document from the current node that match the selection no matter where they are\n",
        "\n",
        ".\tSelects the current node\n",
        "\n",
        "..\tSelects the parent of the current node\n",
        "\n",
        "@\tSelects attributes\n",
        "\n",
        "So...\n",
        "//div (select all div nodes)[@title=\"buyer-name\"](select the title attribute with the value of buyer-name)/text()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tfyo2VMoLR5Q",
        "outputId": "094debf2-e77d-4537-be69-9eb69ce50c32"
      },
      "source": [
        "buyerhs = tree.xpath('//div[@title=\"buyer-name\"]/text()')\n",
        "pricehs = tree.xpath('//span[@class=\"item-price\"]/text()')\n",
        "\n",
        "#print('Buyers: ', buyers)  \n",
        "#print('Prices: ', prices)\n",
        "\n",
        "# code below was added after to test if this shit would work, it did.\n",
        "billy = dict(zip(buyerhs,pricehs))\n",
        "print(billy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Carson Busses': '$29.95', 'Earl E. Byrd': '$8.37', 'Patty Cakes': '$15.26', 'Derri Anne Connecticut': '$19.25', 'Moe Dess': '$19.25', 'Leda Doggslife': '$13.99', 'Dan Druff': '$31.57', 'Al Fresco': '$8.49', 'Ido Hoe': '$14.47', 'Howie Kisses': '$15.86', 'Len Lease': '$11.11', 'Phil Meup': '$15.98', 'Ira Pent': '$16.27', 'Ben D. Rules': '$7.50', 'Ave Sectomy': '$50.85', 'Gary Shattire': '$14.26', 'Bobbi Soks': '$5.68', 'Sheila Takya': '$15.00', 'Rose Tattoo': '$114.07', 'Moe Tell': '$10.09'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VskHLaMBLR5R"
      },
      "source": [
        "### Now we basically created an individual list for each of the 2 request..lets see if we can go deeper\n",
        "\n",
        "We know that the pages are numbered 01 - 05, so we have a range for the sites... range(1, 5)\n",
        "\n",
        "I would like to actually merge the items together into a dictionary, and write them to a text file.\n",
        "\n",
        "Easy way to do this is to pull from Stackoverflow: \n",
        "https://stackoverflow.com/questions/7271385/how-do-i-combine-two-lists-into-a-dictionary-in-python\n",
        "\n",
        "dict(zip([buyers],[prices])) \n",
        "In theory... zip will look at the 2 iterables(lists) and take value 0 from both and output it as a tuple, and it will continue on.  Dict will t ake the tuples and merge them together into a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PURzIOOFLR5R",
        "outputId": "25fe59e9-24e0-4750-dce6-a83ab627a1a9"
      },
      "source": [
        "from lxml import html\n",
        "import requests\n",
        "\n",
        "siteCount = range(1,6) # We start at one and we need to stop after 5\n",
        "\n",
        "SiteNumber = 1\n",
        "# I want to create the file in the current directory,and write my text header, then close the file\n",
        "vendorfilename='Vendors.txt'\n",
        "VendorPricing =open(vendorfilename,'w')\n",
        "VendorPricing.write(\" Vendor:    |     Price: \\n\")\n",
        "VendorPricing.close()\n",
        "\n",
        "# For some reason, request.get picks up mutiple positional arguments, so lets set it as a string\n",
        "\n",
        "while SiteNumber <6:\n",
        "    # Sets up the requests to the site number, since the examples are 001-005\n",
        "    site =['http://econpy.pythonanywhere.com/ex/00',str(SiteNumber),'.html']\n",
        "    requestSite=''.join(site)\n",
        "    page = requests.get(requestSite)\n",
        "    tree = html.fromstring(page.content)\n",
        "    # I point it to the title and class elements, and grab the vendor and pricing data\n",
        "    buyers = tree.xpath('//div[@title=\"buyer-name\"]/text()')\n",
        "    prices = tree.xpath('//span[@class=\"item-price\"]/text()')\n",
        "    # Next, I essentially have 2 lists: [buyers] and [price], and will merge them to a dict\n",
        "    vendorData = dict(zip(buyers,prices))\n",
        "    # since, the file is already created, lets append to it.\n",
        "    vendors = open(vendorfilename,'a')\n",
        "    for k,v in vendorData.items():\n",
        "        vendors.write(f'{k}   |    {v}\\n')\n",
        "    # data is written, close the file\n",
        "    print(\"Data written to file\")\n",
        "    vendors.close()\n",
        "    # Up the site count by 1\n",
        "    SiteNumber+=1\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data written to file\n",
            "Data written to file\n",
            "Data written to file\n",
            "Data written to file\n",
            "Data written to file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2Nn7fI3LR5S"
      },
      "source": [
        "## Shraboom, now we have an idea and it puts out some tasty data:\n",
        "\n",
        "B.V. Dease   |    $11.98 \n",
        "\n",
        "Benny Fitt   |    $16.75\n",
        "\n",
        "Bette R. Haff   |    $8.79\n",
        "\n",
        "Desi Krashum   |    $9.99\n",
        "\n",
        "Gill D. Lily   |    $21.21\n",
        "\n",
        "Cole Mines   |    $11.56\n",
        "\n",
        "Phil R. Monik   |    $8.99\n",
        "\n",
        "L.O. Quency   |    $14.30\n",
        "\n",
        "Connel Radd   |    $4.99\n",
        "\n",
        "Ben D. Rules   |    $45.00\n",
        "\n",
        "Cleon Sheets   |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7qdPM07LR5T"
      },
      "source": [
        "# **BeautifulSoup4** \n",
        "\n",
        "So, next bit here is to use beautifulsoup4, which is installed by default in Anaconda\n",
        "\n",
        "So, the example we will be using is the National weather servicees data.\n",
        "\n",
        "While we could hand copy and paste, that shit sucks...and is reserved for interns.  \n",
        "\n",
        "So, to reiterate above with web scraping, we can use code to filter through the page looking for the HTML elements we specified and extract the content for whatever we want to extract.\n",
        "\n",
        "The guide for it is pulled from their documentation page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-YRDTFnLR5U"
      },
      "source": [
        "html_doc = \"\"\"<html><head><title>The Dormouse's story</title></head>\n",
        "<body>\n",
        "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
        "\n",
        "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
        "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
        "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
        "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
        "and they lived at the bottom of a well.</p>\n",
        "\n",
        "<p class=\"story\">...</p>\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MS_S5p8GLR5U",
        "outputId": "c6965fac-7aa5-492c-c8b6-a97423cc4f71"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "\n",
        "print(soup.prettify())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<html>\n",
            " <head>\n",
            "  <title>\n",
            "   The Dormouse's story\n",
            "  </title>\n",
            " </head>\n",
            " <body>\n",
            "  <p class=\"title\">\n",
            "   <b>\n",
            "    The Dormouse's story\n",
            "   </b>\n",
            "  </p>\n",
            "  <p class=\"story\">\n",
            "   Once upon a time there were three little sisters; and their names were\n",
            "   <a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">\n",
            "    Elsie\n",
            "   </a>\n",
            "   ,\n",
            "   <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">\n",
            "    Lacie\n",
            "   </a>\n",
            "   and\n",
            "   <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">\n",
            "    Tillie\n",
            "   </a>\n",
            "   ;\n",
            "and they lived at the bottom of a well.\n",
            "  </p>\n",
            "  <p class=\"story\">\n",
            "   ...\n",
            "  </p>\n",
            " </body>\n",
            "</html>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xug-FnkLR5U"
      },
      "source": [
        "### Fancy shit, here's ways to navigate it:\n",
        "\n",
        "1. soup.title - <title> The Dormouse's Story </title>\n",
        "\n",
        "2. soup.title.name - u'title'\n",
        "\n",
        "3. soup.title.string - u'The dormouse's Story'\n",
        "\n",
        "4. soup.title.parent.name - u'head'\n",
        "\n",
        "5. soup.p -\" (<)p class=\"title\">(<b>)The Dormouse's story(</b>)(</p>)\"\n",
        "\n",
        "6. soup.p['class'] - u'title'\n",
        "\n",
        "7. soup.a - (snags the first instance of the <a class\n",
        "\n",
        "8. soup.find_all('a') - will return ALL of the (<a) class objects \n",
        "\n",
        "9. soup.find(id=\"link3\"\n",
        "\n",
        "### Extract the URLs found within the page's tags \n",
        "\n",
        "A typical common task is extracting all of the URLs found within a page's <a> tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7StvhjJNXQy3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVtsq-5kLR5V",
        "outputId": "68f8555e-f0c3-426f-a359-fc50ba326748"
      },
      "source": [
        "for link in soup.find_all('a'):\n",
        "    print(link.get('href'))  \n",
        "    # This will basically pull and print t"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://example.com/elsie\n",
            "http://example.com/lacie\n",
            "http://example.com/tillie\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4llmTeYPl2O"
      },
      "source": [
        "## How does it work?\n",
        "BS4  parses the the constructors using a string method.  It first converts it to unicode, and the html entities are converted to unicode characters.  BS4 then parses that data using an HTML parser, or you can tell it to parse using an xml parser.\n",
        "\n",
        "Name - tag.name - The base object (title, head, p, etc)\\\n",
        "\n",
        "Attributes - tag.attrs - A tag can have any number of attributes \\<b class='boldest'> has an attribute 'class' who value is 'boldest.' Anything not a tag is basically an attribute.\\\n",
        "**Note** : Some attributes can have amultiple values.  In these cases, BS4 shows them as a list.  You can consolidate multiple attribute values if you turn the tag into a string.\n",
        "\n",
        "If you need to grab the string part of an attribute, you can just simply return the .string method of a soup object. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c4OQT8CSSoz"
      },
      "source": [
        "### Navigating by Tags\n",
        "You can navigate a document in a few ways.\n",
        "\n",
        "1. Navigate by using tag names (soup.head / soup.title/etc).  If you want specific tags, use that tag (soup.body.b -> returns the first \\<b> tag in the \\<body> tag\\.  Doing this will only get you the first instance of the tag\n",
        "\n",
        "2. If you want all of them, use the find_all() method. (soup.find_all(\"a\"))\\\n",
        "\n",
        "3. If you want the contents and children, we can search the tags children in a list by .contents method.\n",
        "\n",
        "4. descendants attribute allows you to iterate over all of a tag's children, recursively: its direct children, and the children of its direct children and so on.\\\n",
        "\n",
        "5. If a tag has only one child, and that child is a Navigablestring [No. 2 from above] the child is made available as a string\\\n",
        "\n",
        "\n",
        "**Going Up a Tree**\n",
        "You can go up a family tree via the .parent attribute, however; the .parent attribute of a beautifulSoup object is None\n",
        "\n",
        "If you want to iterate over all the parent elements, use .parents.\n",
        "\n",
        "\n",
        "**Going sideways**\n",
        "Use .next_sibling and .previous_sibling to navigate between page elements that are on the same level of the parse tree..\n",
        "\n",
        "If you want to iterate over all of them, use siblings\n",
        "\n",
        "\n",
        "**Next Element**\n",
        ".next_element points to whatever was parsed immediately afterwards.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLwNObclbcsy"
      },
      "source": [
        "## Searching through the tree\n",
        "\n",
        "You can search through a tree in a number of ways besides find/find_all\n",
        "\n",
        "1. string -> passing a string to the search method and BS4 will perform a match against the exact string (soup.findall('x')\n",
        "\n",
        "2.  Regex...Yep, that works with BS\n",
        "\n",
        "3. List -> You can pass mutiple tags to find by providing a list. (markup.findall(['pre','b']))\n",
        "\n",
        "4. True -> True will return all tags that it can find, no strings on their own\n",
        "\n",
        "5. find_all() -> You can use find_all() to extract all the occurrences of a particular tag from the page\n",
        "  (find_all(name, attrs, recursive, string, limit, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsCeK56fSSUB"
      },
      "source": [
        "## BeautifulSoup4 tutorial\n",
        "This is pulled from the tutorial book found online\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khMP0kiRN6_B"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "url = \"https://www.tutorialspoint.com/index.htm\"\n",
        "req = requests.get(url)\n",
        "soup = BeautifulSoup(req.text, \"html.parser\")\n",
        "print(soup.title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v66B3ICmOZG2"
      },
      "source": [
        "## Common uses and ideas:\n",
        ">>\n",
        " Pulling all of the URLs in a web page:\n",
        "\n",
        " for link in soup.find_all('a'):\\\n",
        "    print(link.get('href'))\n",
        ">>\n",
        "Finds all 'a' nodes and prints their href attribute value.  'a' nodes are used for links and are proceeded by an href"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcYlpSuWOWnT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpONMrD_XJty"
      },
      "source": [
        "\n",
        "# Automate The Boring Stuff - Web scraping\n",
        "\n",
        "Automate the boring stuff also has a section on web scraping that covers some of the stuff above.  Ultimatley, I plan to merge this all together into a master notebook for eventual use.\n",
        "\n",
        "Automate the boring stuff uses a handful of modules, some new, some old:\n",
        "---\n",
        "webbrowser - Comes standard with Python and opens a browser to a specific page\\\n",
        "requests - covered above, but can be used to download files and web pages from the internet\n",
        "bs4 - Parses HTML, the format that web pages are written in \\\n",
        "selenimum  -  Launches and controls a web browser.  This module is able to fill in forms and simulate mouse clicks inside the browser\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yes893iJYoAZ",
        "outputId": "dc501e8e-3f8c-4c1f-e7bf-7579f1695d53"
      },
      "source": [
        "# Lets try some shit out\n",
        "import webbrowser\n",
        "webbrowser.open('htps://inventwithpython.com/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KFQyXaBwpA1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Z3VPJ3wpA1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFGCaQUkwpA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae7d005-2b92-40a2-80af-d2447fa06248"
      },
      "source": [
        "import webbrowser,sys\n",
        "\n",
        "street = str(input(\"What is the street address?\\n\"))\n",
        "city = str(input(\"What is the city?\\n\"))\n",
        "state = str(input(\"What is the state?\\n\"))\n",
        "\n",
        "googlemap=str(f'https://www.google.com/maps/place/{street}+{city}+{state}')\n",
        "webbrowser.open(googlemap)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "What is the street address?\n",
            "306\n",
            "What is the city?\n",
            "9th street\n",
            "What is the state?\n",
            "MS\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_otHzp_wpA2"
      },
      "source": [
        "## Request Module and downloading data\n",
        "\n",
        "So this wasn't touched on earlier.  The requests module was used to grab and import data into a JSON for cleaning.\n",
        "The module itself lets you download files from the web without having to deal with the general issues.  It works akin to a curl requests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgZsVJNgwpA2",
        "outputId": "2025df06-01cc-4a84-e1e1-c33355876a82"
      },
      "source": [
        "import requests\n",
        "res = requests.get('https://automatetheboringstuff.com/files/rj.txt')\n",
        "type(res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "requests.models.Response"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipmbKjRUwpA2",
        "outputId": "bb47c83e-fd90-4b33-b61f-7d7113f1ef38"
      },
      "source": [
        "len(res.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "178978"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFwaV6b9wpA3",
        "outputId": "f0873b9a-323a-4c0f-b800-6ecc46dec436"
      },
      "source": [
        "print(res.text[:250])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare\n",
            "\n",
            "This eBook is for the use of anyone anywhere at no cost and with\n",
            "almost no restrictions whatsoever.  You may copy it, give it away or\n",
            "re-use it under the terms of the Projec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDJUd21iwpA3"
      },
      "source": [
        "The URL above grabs a text file of the Romeo and Julie play from the specified site.\n",
        "\n",
        "If we want to save the object we do the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTRfQp3bwpA3"
      },
      "source": [
        "import requests\n",
        "res = requests.get('https://automatetheboringstuff.com/files/rj.txt')\n",
        "\n",
        "res.raise_for_status()\n",
        "playFile = open('RomeoAndJuliet.txt','wb')\n",
        "for chunk in res.iter_content(100000):\n",
        "    playFile.write(chunk)\n",
        "    \n",
        "    \n",
        "playFile.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRX2E0rlwpA3"
      },
      "source": [
        "So when we write the data to a file, we have write it in write binary mode (wb).  This has to be done in order to maintain the Unicode encoding of the text.  Without this, the data is written in data form rather than human readable form.\n",
        "\n",
        "The iter_content() method returns data 'chunks' of the content on each iteration through the loop, and you specify how many bytes each chunk will contain.  100k represents about 100KB, so we pass that as the argument for iter_content().\n",
        "\n",
        "That's pretty much it.\n",
        "\n",
        "1. Call requests.get() to download the file\n",
        "2. Call open() with 'wb' to create a new file in write binary mode\n",
        "3. Loop over the Response object's iter_content() method\n",
        "4. Call write() on each iteration to write the content to the file\n",
        "5. Call close() to close the file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb2P27ezwpA3"
      },
      "source": [
        "## Parsing HTML with BS4\n",
        "\n",
        "\n",
        "Beautiful Soup 4 is a module designed to extract information from an HTML page.  This code will use the requests.get() module to download th emain page from the No Starch Press website and passes the text attribute of the response to bs4.BeautifulSoup().  this object that it returns is stored in a variable named noStarchSoup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_32gmbR7wpA4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d312ee0c-7f09-4c8c-fc1d-91b704f2f8e6"
      },
      "source": [
        "import requests, bs4\n",
        "\n",
        "res = requests.get('https://nostarch.com')\n",
        "res.raise_for_status()\n",
        "\n",
        "noStarchSoup = bs4.BeautifulSoup(res.text,'html.parser')\n",
        "type(noStarchSoup)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "bs4.BeautifulSoup"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_Ll6iqcaz3w"
      },
      "source": [
        "## Parsing National Weather service map for data\n",
        "The book calls for you to use an example html file but I opted to do this instead.\n",
        "\n",
        "So basically below we will use the BS4 module to parse data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdPk7GWBY2sr"
      },
      "source": [
        "import requests, bs4\n",
        "\n",
        "res = requests.get(\"https://forecast.weather.gov/MapClick.php?lat=39.9736&lon=-74.6824\")\n",
        "\n",
        "res.raise_for_status()\n",
        "weather = bs4.BeautifulSoup(res.text, 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpIt4_V-bDZ4"
      },
      "source": [
        "soup.select('object') - returns all elements named object.  This will return the opening and closing of the object.\n",
        "\n",
        "soup.select('#author') = will look look for the element with an id attribute of author\n",
        "\n",
        "soup.select('.notice') - All elements that use a CSS class attribute named notice\n",
        "\n",
        "soup.select('div > span') -- All elements named span that are directly within an element named div, with no element in between.\n",
        "\n",
        "soup.select('input[name]') - All elements named input that have a name attribute with any value\n",
        "\n",
        "soup.select('input[type=\"buton\"] - All elements named input that have an attribute named type with value button.\n",
        "\n",
        "\n",
        "Instead of writing the elements, you can just get lazy and copy the xpath\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRhEgsagep9s",
        "outputId": "975f170a-ee65-40f4-81b7-a91d4c509158"
      },
      "source": [
        "import requests, bs4\n",
        "\n",
        "res = requests.get(\"https://forecast.weather.gov/MapClick.php?lat=39.9736&lon=-74.6824\")\n",
        "\n",
        "res.raise_for_status()\n",
        "weather = bs4.BeautifulSoup(res.text, 'html.parser')\n",
        "\n",
        "div = weather.select('div') # Will select all of the div elements\n",
        "type(div)\n",
        "print(div[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<div class=\"container-fluid\">\n",
            "<div class=\"navbar-header\">\n",
            "<button class=\"navbar-toggle collapsed\" data-target=\"#top-nav\" data-toggle=\"collapse\" type=\"button\">\n",
            "<span class=\"sr-only\">Toggle navigation</span>\n",
            "<span class=\"icon-bar\"></span>\n",
            "<span class=\"icon-bar\"></span>\n",
            "<span class=\"icon-bar\"></span>\n",
            "</button>\n",
            "</div>\n",
            "<div class=\"collapse navbar-collapse\" id=\"top-nav\">\n",
            "<ul class=\"nav navbar-nav\">\n",
            "<li><a href=\"http://www.weather.gov\">HOME</a></li>\n",
            "<li class=\"dropdown\"><a class=\"dropdown-toggle\" data-toggle=\"dropdown\" href=\"http://www.weather.gov/forecastmaps\">FORECAST<span class=\"caret\"></span></a><ul class=\"dropdown-menu\" role=\"menu\"><li><a href=\"http://www.weather.gov\">Local</a></li><li><a href=\"http://digital.weather.gov\">Graphical</a></li><li><a href=\"http://www.aviationweather.gov/\">Aviation</a></li><li><a href=\"http://www.nws.noaa.gov/om/marine/home.htm\">Marine</a></li><li><a href=\"http://water.weather.gov/ahps/\">Rivers and Lakes</a></li><li><a href=\"http://www.nhc.noaa.gov/\">Hurricanes</a></li><li><a href=\"http://www.spc.noaa.gov/\">Severe Weather</a></li><li><a href=\"http://www.weather.gov/fire/\">Fire Weather</a></li><li><a href=\"https://www.esrl.noaa.gov/gmd/grad/solcalc/sunrise.html\">Sun/Moon</a></li><li><a href=\"http://www.cpc.ncep.noaa.gov/\">Long Range Forecasts</a></li><li><a href=\"http://www.cpc.ncep.noaa.gov\">Climate Prediction</a></li><li><a href=\"https://www.swpc.noaa.gov/\">Space Weather</a></li></ul> </li>\n",
            "<li class=\"dropdown\"><a class=\"dropdown-toggle\" data-toggle=\"dropdown\" href=\"https://w2.weather.gov/climate\">PAST WEATHER<span class=\"caret\"></span></a><ul class=\"dropdown-menu\" role=\"menu\"><li><a href=\"https://w2.weather.gov/climate/\">Past Weather</a></li><li><a href=\"https://w2.weather.gov/climate/\">Heating/Cooling Days</a></li><li><a href=\"https://w2.weather.gov/climate/\">Monthly Temperatures</a></li><li><a href=\"https://w2.weather.gov/climate/\">Records</a></li><li><a href=\"http://aa.usno.navy.mil/\">Astronomical Data</a></li></ul> </li>\n",
            "<li class=\"dropdown\"><a class=\"dropdown-toggle\" data-toggle=\"dropdown\" href=\"http://www.weather.gov/safety\">SAFETY<span class=\"caret\"></span></a><ul class=\"dropdown-menu\" role=\"menu\"><li><a href=\"https://www.weather.gov/safety/flood\">Floods</a></li><li><a href=\"https://www.weather.gov/safety/tsunami\">Tsunamis</a></li><li><a href=\"https://www.weather.gov/safety/beachhazards\">Beach Hazards</a></li><li><a href=\"https://www.weather.gov/safety/wildfire\">Wildfire</a></li><li><a href=\"https://www.weather.gov/safety/cold\">Cold</a></li><li><a href=\"https://www.weather.gov/safety/tornado\">Tornadoes</a></li><li><a href=\"https://www.weather.gov/safety/airquality\">Air Quality</a></li><li><a href=\"https://www.weather.gov/safety/fog\">Fog</a></li><li><a href=\"https://www.weather.gov/safety/heat\">Heat</a></li><li><a href=\"https://www.weather.gov/safety/hurricane\">Hurricanes</a></li><li><a href=\"https://www.weather.gov/safety/lightning\">Lightning</a></li><li><a href=\"https://www.weather.gov/safety/safeboating\">Safe Boating</a></li><li><a href=\"https://www.weather.gov/safety/ripcurrent\">Rip Currents</a></li><li><a href=\"https://www.weather.gov/safety/thunderstorm\">Thunderstorms</a></li><li><a href=\"https://www.weather.gov/safety/space\">Space Weather</a></li><li><a href=\"https://www.weather.gov/safety/heat-uv\">Sun (Ultraviolet Radiation)</a></li><li><a href=\"http://www.weather.gov/safetycampaign\">Safety Campaigns</a></li><li><a href=\"https://www.weather.gov/safety/wind\">Wind</a></li><li><a href=\"https://www.weather.gov/safety/drought\">Drought</a></li><li><a href=\"https://www.weather.gov/safety/winter\">Winter Weather</a></li></ul> </li>\n",
            "<li class=\"dropdown\"><a class=\"dropdown-toggle\" data-toggle=\"dropdown\" href=\"http://www.weather.gov/informationcenter\">INFORMATION<span class=\"caret\"></span></a><ul class=\"dropdown-menu\" role=\"menu\"><li><a href=\"http://www.weather.gov/Owlie's\">Owlie's Kids Page</a></li><li><a href=\"http://www.weather.gov/wrn/wea\">Wireless Emergency Alerts</a></li><li><a href=\"https://www.weather.gov/owlie/publication_brochures\">Brochures</a></li><li><a href=\"http://www.weather.gov/wrn/\">Weather-Ready Nation</a></li><li><a href=\"https://www.weather.gov/coop/\">Cooperative Observers</a></li><li><a href=\"http://www.weather.gov/briefing/\">Daily Briefing</a></li><li><a href=\"http://www.nws.noaa.gov/om/hazstats.shtml\">Damage/Fatality/Injury Statistics</a></li><li><a href=\"http://mag.ncep.noaa.gov/\">Forecast Models</a></li><li><a href=\"https://www.weather.gov/gis\">GIS Data Portal</a></li><li><a href=\"https://www.weather.gov/nwr/\">NOAA Weather Radio</a></li><li><a href=\"http://weather.gov/publications\">Publications</a></li><li><a href=\"http://www.weather.gov/SKYWARN\">SKYWARN Storm Spotters</a></li><li><a href=\"http://www.weather.gov/StormReady\">StormReady</a></li><li><a href=\"https://www.weather.gov/TsunamiReady/\">TsunamiReady</a></li><li><a href=\"https://www.weather.gov/notification/\">Service Change Notices</a></li></ul> </li>\n",
            "<li class=\"dropdown\"><a class=\"dropdown-toggle\" data-toggle=\"dropdown\" href=\"http://www.weather.gov/owlie\">EDUCATION<span class=\"caret\"></span></a><ul class=\"dropdown-menu\" role=\"menu\"><li><a href=\"https://www.weather.gov/wrn/force\">Be A Force of Nature</a></li><li><a href=\"http://www.weather.gov/owlie\">NWS Education Home</a></li></ul> </li>\n",
            "<li class=\"dropdown\"><a class=\"dropdown-toggle\" data-toggle=\"dropdown\" href=\"http://www.weather.gov/contact-media/\">NEWS<span class=\"caret\"></span></a><ul class=\"dropdown-menu\" role=\"menu\"><li><a href=\"http://www.weather.gov/news\">NWS News</a></li><li><a href=\"https://www.weather.gov/wrn/calendar\">Events</a></li><li><a href=\"http://www.weather.gov/socialmedia\">Social Media</a></li><li><a href=\"https://www.weather.gov/owlie/publication_brochures\">Pubs/Brochures/Booklets </a></li><li><a href=\"http://www.noaa.gov/NOAA-Communications\">NWS Media Contacts</a></li></ul> </li>\n",
            "<li class=\"dropdown\"><a class=\"dropdown-toggle\" data-toggle=\"dropdown\" href=\"http://www.weather.gov/search\">SEARCH<span class=\"caret\"></span></a><ul class=\"dropdown-menu\" role=\"menu\"> <li><!-- Begin search code -->\n",
            "<div id=\"site-search\">\n",
            "<form action=\"//search.usa.gov/search\" method=\"get\" style=\"margin-bottom: 0; margin-top: 0;\">\n",
            "<input name=\"v:project\" type=\"hidden\" value=\"firstgov\"/>\n",
            "<label for=\"query\">Search For</label>\n",
            "<input id=\"query\" name=\"query\" size=\"12\" type=\"text\"/>\n",
            "<input type=\"submit\" value=\"Go\"/>\n",
            "<p>\n",
            "<input checked=\"checked\" id=\"nws\" name=\"affiliate\" type=\"radio\" value=\"nws.noaa.gov\"/>\n",
            "<label class=\"search-scope\" for=\"nws\">NWS</label>\n",
            "<input id=\"noaa\" name=\"affiliate\" type=\"radio\" value=\"noaa.gov\"/>\n",
            "<label class=\"search-scope\" for=\"noaa\">All NOAA</label>\n",
            "</p>\n",
            "</form>\n",
            "</div>\n",
            "</li>\n",
            "</ul> </li>\n",
            "<li class=\"dropdown\"><a class=\"dropdown-toggle\" data-toggle=\"dropdown\" href=\"http://www.weather.gov/about\">ABOUT<span class=\"caret\"></span></a><ul class=\"dropdown-menu\" role=\"menu\"><li><a href=\"http://www.weather.gov/about\">About NWS</a></li><li><a href=\"http://www.weather.gov/organization\">Organization</a></li><li><a href=\"https://www.weather.gov/media/wrn/NWS_Weather-Ready-Nation_Strategic_Plan_2019-2022.pdf\">Strategic Plan</a></li><li><a href=\"https://sites.google.com/a/noaa.gov/nws-insider/\">For NWS Employees</a></li><li><a href=\"http://www.weather.gov/international/\">International</a></li><li><a href=\"http://www.weather.gov/organization\">National Centers</a></li><li><a href=\"http://www.weather.gov/careers/\">Careers</a></li><li><a href=\"http://www.weather.gov/contact\">Contact Us</a></li><li><a href=\"https://w1.weather.gov/glossary\">Glossary</a></li></ul> </li>\n",
            "</ul>\n",
            "</div>\n",
            "</div>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqqNTRibZZNz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_B_wxiBvPCe"
      },
      "source": [
        "## Project : Opening all search results from google\n",
        "  - Done in Pycharm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM1csE49vaBo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}